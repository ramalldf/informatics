{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginia'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states['VA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_file= open(\"AFINN-111.txt\")\n",
    "afinnfile = open(\"AFINN-111.txt\")\n",
    "scores = {} # initialize an empty dictionary\n",
    "for line in afinnfile:\n",
    "  term, score  = line.split(\"\\t\")  # The file is tab-delimited. \"\\t\" means \"tab character\"\n",
    "  scores[term] = int(score)  # Convert the score to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tweet_file= open('output.txt', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open file 'q2_10.txt', mode 'r' at 0x000000000311DDB0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetScores= []#Initialize list of total score per tweet\n",
    "\n",
    "tweet_index= 0#Initialize a counter to keep track of tweet we're on\n",
    "for line2 in tweet_file:\n",
    "    try:#We'll try our entire process for all objects that are tweets and not 'delete' objects (which have no 'text' field)\n",
    "        tweet_index= tweet_index+1\n",
    "        dataDict= json.loads(line2)#Loads first 'line2', which is actually the first json object\n",
    "        #print 'Tweet index:', tweet_index, ',', 'Tweet text: '#, dataDict['text']\n",
    "        tweet= dataDict['text'].split()#Split text object into a list of terms\n",
    "        print tweet\n",
    "        \n",
    "        tweetList= []#Initialize list of scores for tweet \n",
    "        sumTweet= []#Initialize sum variable\n",
    "        for i in range(0,len(tweet)):#For item in list, if it is in dictionary, append value/score (from key:value pair) to tweetList. If not, append a 0 value instead\n",
    "            if tweet[i].encode('utf-8').strip() in scores.keys():\n",
    "                 tweetList.append(scores[tweet[i]])\n",
    "            else:\n",
    "                tweetList.append(0)\n",
    "        sumTweet= sum(tweetList)#Add scores of all terms in tweet\n",
    "        tweetScores.append(sumTweet)\n",
    "        print sumTweet#'Tweet score: ', sumTweet#Append this total score to a list with combined scores for all tweets\n",
    "    except:#If not, then:\n",
    "        print 'Has no text, skip entry'\n",
    "        continue\n",
    "    \n",
    "    #==\n",
    "    #The following is not relevant anymore since it prints out 0 as length (reason is we finished reading file so by this point there are no more lines left)\n",
    "    #lines(sent_file)\n",
    "    #lines(tweet_file)\n",
    "    #print 'Our final score table for the tweets is: ', tweetScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def term_list(tweet_file):\n",
    "    '''This function will open a tweet file, split the content of each \n",
    "    tweets text into a list and append the lists of multiple lists into a master list '''\n",
    "    \n",
    "    #output= open(tweet_file, 'r')\n",
    "    tweet_index= 0#Initialize a counter to keep track of tweet we're on\n",
    "    total_terms= []\n",
    "    for line in tweet_file:\n",
    "        try:#We'll try our entire process for all objects that are tweets and not 'delete' objects (which have no 'text' field)\n",
    "            #print 'working'\n",
    "            tweet_index= tweet_index+1\n",
    "            dataDict= json.loads(line)#Loads first 'line2', which is actually the first json object\n",
    "            tweet= dataDict['text'].split()#Split text object into a list of terms            \n",
    "            total_terms.append(tweet)\n",
    "            \n",
    "        except:#If not, then:\n",
    "            print 'Has no text, skip entry'\n",
    "            continue\n",
    "    #Append tweet lists to a larger list (as long as they're larger than one character)\n",
    "    one_list= [item.encode('utf-8') for sublist1 in total_terms for item in sublist1 if len(item)>1]\n",
    "    return one_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hw question #4: Determining the happiest state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will determine a cumulative sentiment score for US states. We will need to extract both the sentiment for every tweet, but also the location. We'll then aggregate the locations sentiment scores and determine the average sentiment score.\n",
    "\n",
    "We do this by: 1) Collecting user:location and tweet data 2) Clean data so only tweets with a legit state is counted 3) Make new list of tweet sentiment score, same length as clean set 4) Append scores to a dictionary with state scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tweet_file= open('q1.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweet_state(tweet_file):\n",
    "    '''This function will open a tweet file, split the content of each \n",
    "    tweets text into a tweet list and a location list. '''  \n",
    "    tweet_index= 0#Initialize a counter to keep track of tweet we're on\n",
    "    loc_list= []#Initialize lists\n",
    "    tweet_list= []\n",
    "    for line in tweet_file:\n",
    "            tweet_index= tweet_index+1\n",
    "            dataDict= json.loads(line)#Loads first 'line2', which is actually the first json object\n",
    "            if \"user\" in dataDict.keys():#Look for 'user' field\n",
    "                userField= dataDict[\"user\"]#Dump contents into userField and location objects and append to list     \n",
    "                location= userField[\"location\"]\n",
    "                loc_list.append(location)\n",
    "                \n",
    "                tweet= dataDict['text']#Do the same thing for the tweets\n",
    "                tweet_list.append(tweet)\n",
    "    return loc_list, tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stateTweets = tweet_state(tweet_file)#Generate list of states and corresponding tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clean up lists to remove items with no locations\n",
    "cleanState= [stateTweets[0][i] for i in range(0,len(stateTweets[0])) if stateTweets[0][i] is not None]\n",
    "cleanTweet= [stateTweets[1][i] for i in range(0,len(stateTweets[0])) if stateTweets[0][i] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 315\n"
     ]
    }
   ],
   "source": [
    "#Validate that we have the correct length cleaned lists\n",
    "print len(cleanState), len(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findState(txtString):\n",
    "    '''Function takes a the location string and splits into two to get the state'''\n",
    "    try:\n",
    "        parseStr= txtString.split(', ')\n",
    "        return parseStr[1]\n",
    "    except:\n",
    "        pass#print 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleTweetSent(tweet, sentiment_file):\n",
    "    '''This fxn takes a tweet, converts it to a string, then\n",
    "    splits it into a list of terms. We then use the AFINN-111 \n",
    "    sentiment dictionary, we then assign scores to all the terms\n",
    "    and take their sum to get a cumulative tweet sentiment score'''\n",
    "    #Create dictionary for sentiment\n",
    "    afinnfile = sentiment_file\n",
    "    scores = {} # initialize an empty dictionary\n",
    "    for line in afinnfile:\n",
    "        term, score  = line.split(\"\\t\")  # The file is tab-delimited. \"\\t\" means \"tab character\"\n",
    "        scores[term] = int(score)  # Convert the score to an integer.\n",
    "    \n",
    "    tweet_score= []\n",
    "    split_tweet= tweet.encode('utf-8').split()\n",
    "    for i in range(0,len(split_tweet)):\n",
    "        if split_tweet[i] in scores.keys():\n",
    "            tweet_score.append(scores[split_tweet[i]])\n",
    "        else:\n",
    "            tweet_score.append(0)\n",
    "    return sum(tweet_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This extracts the state location from a tweet and calculates the corresponding tweet sentiment for it\n",
    "#Point of hw is to find the average value for each state so we'll need to keep track of ALL values for all states\n",
    "#We'll initialize a state:score dictionary\n",
    "#We'll calculate the averages later\n",
    "stateList= []#We'll keep these for reference\n",
    "sentimentList= []\n",
    "stateScoreDict= {key: list() for key in states}#Initialize dictionary of states and scores with dict comprehension\n",
    "for i in range(0,len(cleanState)):\n",
    "    state= findState(cleanState[i])#Split line into (city, state) and recover state string if available\n",
    "    if state in states.keys():#Search for state abbrev. in dictionary keys list\n",
    "        state= str(state.upper())\n",
    "        tweet_sent= singleTweetSent(cleanTweet[i])#Recovers tweet text\n",
    "        stateList.append(state)#Appends state and tweets to respective lists\n",
    "        sentimentList.append(tweet_sent)\n",
    "        stateScoreDict[state].append(tweet_sent)#Append tweet sentiment score to list of scores for appropriate state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['WA'], 6.0)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The final step is to consolidate the values of our score dictionary into a new dictionary\n",
    "#with average sentiment scores for each state (that had at least one score)\n",
    "stateScoreAvg= {key: (float(sum(stateScoreDict[key]))/len(stateScoreDict[key])) for key in stateScoreDict if len(stateScoreDict[key]) > 0}\n",
    "#Then we'll find the max score in our new dictionary\n",
    "maxVal= max(stateScoreAvg.values())#Turn values into a list, return max value in that list\n",
    "[i for i in stateScoreAvg.keys() if stateScoreAvg[i] == maxVal], maxVal#Return key with equivalent value to max value and max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AZ': 0.0,\n",
       " 'CA': 1.0,\n",
       " 'CT': 0.0,\n",
       " 'IL': 0.0,\n",
       " 'MI': 3.0,\n",
       " 'MN': 0.0,\n",
       " 'MO': 0.0,\n",
       " 'NC': 0.0,\n",
       " 'NY': 0.0,\n",
       " 'OH': 0.0,\n",
       " 'OR': 0.0,\n",
       " 'SC': 0.0,\n",
       " 'VA': 0.0,\n",
       " 'WA': 6.0}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateScoreAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxStateDict['WV'].append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AK': [],\n",
       " 'AL': [],\n",
       " 'AR': [],\n",
       " 'AS': [],\n",
       " 'AZ': [],\n",
       " 'CA': [],\n",
       " 'CO': [],\n",
       " 'CT': [],\n",
       " 'DC': [],\n",
       " 'DE': [],\n",
       " 'FL': [],\n",
       " 'GA': [],\n",
       " 'GU': [],\n",
       " 'HI': [],\n",
       " 'IA': [],\n",
       " 'ID': [],\n",
       " 'IL': [],\n",
       " 'IN': [],\n",
       " 'KS': [],\n",
       " 'KY': [],\n",
       " 'LA': [],\n",
       " 'MA': [],\n",
       " 'MD': [],\n",
       " 'ME': [],\n",
       " 'MI': [],\n",
       " 'MN': [],\n",
       " 'MO': [],\n",
       " 'MP': [],\n",
       " 'MS': [],\n",
       " 'MT': [],\n",
       " 'NA': [],\n",
       " 'NC': [],\n",
       " 'ND': [],\n",
       " 'NE': [],\n",
       " 'NH': [],\n",
       " 'NJ': [],\n",
       " 'NM': [],\n",
       " 'NV': [],\n",
       " 'NY': [],\n",
       " 'OH': [],\n",
       " 'OK': [],\n",
       " 'OR': [],\n",
       " 'PA': [],\n",
       " 'PR': [],\n",
       " 'RI': [],\n",
       " 'SC': [],\n",
       " 'SD': [],\n",
       " 'TN': [],\n",
       " 'TX': [],\n",
       " 'UT': [],\n",
       " 'VA': [],\n",
       " 'VI': [],\n",
       " 'VT': [],\n",
       " 'WA': [],\n",
       " 'WI': [],\n",
       " 'WV': [0, 4],\n",
       " 'WY': []}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxStateDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has no text, skip entry\n",
      "Has no text, skip entry\n",
      "Has no text, skip entry\n"
     ]
    }
   ],
   "source": [
    "#Now that we have a list of all the terms in our list, we can filter it to get all the unique terms and turn it back to a list\n",
    "one_list= term_list('problem_1_submission.txt')\n",
    "uniqueList= set(one_list)#Filters list to include only unique terms\n",
    "uniqueList= list(uniqueList)#Turn the unique set back into a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We'll make a new list full of zero integers that we'll add to anytime we find an occurrence of our terms\n",
    "#We'll then zip this zero list to our list of unique terms, and turn that into a dictionary\n",
    "size= len(uniqueList)\n",
    "scoreList= [0.0]*size\n",
    "uniqueScore= dict(zip(uniqueList,scoreList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's 0.0160427807487\n",
      "good 0.0160427807487\n",
      "having 0.0160427807487\n",
      "kuya 0.0160427807487\n",
      "around, 0.0160427807487\n",
      "medyo 0.0160427807487\n",
      "nakakawala 0.0160427807487\n",
      "ng 0.0160427807487\n",
      "lungkot. 0.0160427807487\n",
      "@piraco75@MarkGKirshnerCuando 0.0160427807487\n",
      "un 0.0374331550802\n",
      "❤es 0.0160427807487\n",
      "herido,es 0.0160427807487\n",
      "difícil 0.0160427807487\n",
      "confortarlo, 0.0160427807487\n",
      "siempre 0.0267379679144\n",
      "habrá 0.0160427807487\n",
      "un 0.0427807486631\n",
      "nuevo 0.0267379679144\n",
      "amanecer 0.0160427807487\n",
      "un 0.048128342246\n",
      "nuevo 0.0320855614973\n",
      "amor❤ 0.0160427807487\n",
      "https://t.co/grq1lbfwjp 0.0160427807487\n",
      "https://t.co/kMes28OUhn 0.0160427807487\n",
      "満開🌸🌸🌸🌸🌸🌸🌸 0.0160427807487\n",
      "https://t.co/QO7OtAbsnZ 0.0160427807487\n",
      "RT 0.0909090909091\n",
      "@Scra_15: 0.0160427807487\n",
      "Recuerdos 0.0160427807487\n",
      "felices 0.0160427807487\n",
      "que 0.0267379679144\n",
      "en 0.0160427807487\n",
      "mi 0.0160427807487\n",
      "mente 0.0160427807487\n",
      "siempre 0.0320855614973\n",
      "perduraran 0.0160427807487\n",
      "من 0.0160427807487\n",
      "كان 0.0160427807487\n",
      "آخر 0.0160427807487\n",
      "كلامه 0.0160427807487\n",
      "لا 0.0160427807487\n",
      "إله 0.0160427807487\n",
      "إلا 0.0160427807487\n",
      "الله 0.0267379679144\n",
      "دخل 0.0160427807487\n",
      "الجنة 0.0160427807487\n",
      "https://t.co/EbRe8Pke9e 0.0160427807487\n",
      "RT 0.096256684492\n",
      "@Infographic_ksa: 0.0160427807487\n",
      "#موشنجرافيك 0.0160427807487\n",
      "لبعض 0.0160427807487\n",
      "ماورد 0.0160427807487\n",
      "في 0.0160427807487\n",
      "حوار 0.0160427807487\n",
      "⁧#الأمير_محمد_بن_سلمان⁩ 0.0160427807487\n",
      "لبلومبيرغ 0.0160427807487\n",
      "https://t.co/fsn8hE7Wiu 0.0160427807487\n",
      "@pabloamaralgv 0.0160427807487\n",
      "programa 0.0160427807487\n",
      "RT 0.101604278075\n",
      "@Tribute_TFlash: 0.0160427807487\n",
      "@SeriesBrazil 0.0160427807487\n",
      "glee 0.0160427807487\n",
      "pisa 0.0160427807487\n",
      "RT 0.106951871658\n",
      "@TasbehEstigfar: 0.0160427807487\n",
      "تطبيق 0.0160427807487\n",
      "القرآن 0.0160427807487\n",
      "الكريم 0.0160427807487\n",
      "الشامل 0.0160427807487\n",
      "وقف 0.0160427807487\n",
      "وصدقة 0.0160427807487\n",
      "جارية 0.0160427807487\n",
      "للوالد 0.0160427807487\n",
      "الله 0.0320855614973\n",
      "يرحمه، 0.0160427807487\n",
      "لاتحرموه 0.0160427807487\n",
      "الاجر 0.0160427807487\n",
      "https://t.co/pRXVzL90iZ 0.0160427807487\n",
      "ربَي 0.0160427807487\n",
      "ارحمه 0.0160427807487\n",
      "وارحم 0.0160427807487\n",
      "أ… 0.0160427807487\n",
      "Tá 0.0160427807487\n",
      "tendo 0.0160427807487\n",
      "um 0.0160427807487\n",
      "tanto 0.0160427807487\n",
      "de 0.048128342246\n",
      "notícias 0.0160427807487\n",
      "boas 0.0160427807487\n",
      "na 0.0160427807487\n",
      "internet, 0.0160427807487\n",
      "quando 0.0160427807487\n",
      "vou 0.0160427807487\n",
      "abrir 0.0160427807487\n",
      "ler, 0.0160427807487\n",
      "pegadinha 0.0160427807487\n",
      "de 0.0534759358289\n",
      "1° 0.0160427807487\n",
      "de 0.0588235294118\n",
      "abril 0.0160427807487\n",
      "RT 0.112299465241\n",
      "@argiro1961: 0.0160427807487\n",
      "Van 0.0160427807487\n",
      "500 0.0160427807487\n",
      "millones 0.0160427807487\n",
      "por 0.0160427807487\n",
      "Otoniel 0.0160427807487\n",
      "necesito 0.0160427807487\n",
      "la 0.0160427807487\n",
      "red 0.0160427807487\n",
      "de 0.0641711229947\n",
      "cooperantes 0.0160427807487\n",
      "tenía 0.0160427807487\n",
      "el 0.0160427807487\n",
      "guerrerista 0.0160427807487\n",
      "Uribe. 0.0160427807487\n",
      "https://t.co/cdbhlbJRti 0.0160427807487\n",
      "Que 0.0160427807487\n",
      "nunca 0.0160427807487\n",
      "me 0.0160427807487\n",
      "falten 0.0160427807487\n",
      "mis 0.0160427807487\n",
      "amigos 0.0160427807487\n",
      "plis 0.0160427807487\n",
      "👌✌💕 0.0160427807487\n",
      "RT 0.117647058824\n",
      "@Laker_Show: 0.0160427807487\n",
      "REPORT: 0.0160427807487\n",
      "Lakers 0.0160427807487\n",
      "looking 0.0160427807487\n",
      "to 0.0160427807487\n",
      "trade 0.0160427807487\n",
      "D'Angelo 0.0160427807487\n",
      "Russell 0.0160427807487\n",
      "because 0.0160427807487\n",
      "of 0.0160427807487\n",
      "recent 0.0160427807487\n",
      "issues. 0.0160427807487\n",
      "https://t.co/Bq9KmHhYuo 0.0160427807487\n",
      "RT 0.122994652406\n",
      "@etthyene: 0.0160427807487\n",
      "The100 0.0160427807487\n",
      "série 0.0160427807487\n",
      "mais 0.0160427807487\n",
      "real 0.0160427807487\n",
      "que 0.0320855614973\n",
      "tem 0.0160427807487\n",
      "ne: 0.0160427807487\n",
      "lgbt 0.0160427807487\n",
      "só 0.0267379679144\n",
      "toma 0.0160427807487\n",
      "no 0.0160427807487\n",
      "cu, 0.0160427807487\n",
      "mulher 0.0160427807487\n",
      "só 0.0320855614973\n",
      "sofre 0.0160427807487\n",
      "homem 0.0160427807487\n",
      "faz 0.0160427807487\n",
      "merda 0.0160427807487\n",
      "fica 0.0160427807487\n",
      "impune 0.0160427807487\n",
      "pintado 0.0160427807487\n",
      "como 0.0160427807487\n",
      "h… 0.0160427807487\n",
      "RT 0.128342245989\n",
      "@LaurenMJSpain: 0.0160427807487\n",
      "#Harmonizers 0.0160427807487\n",
      "#iHeartAwards 0.0160427807487\n",
      "#BestFanArmy 0.0160427807487\n",
      "https://t.co/EDWnpUPIRk 0.0160427807487\n",
      "tenho 0.0160427807487\n",
      "mania 0.0160427807487\n",
      "em 0.0374331550802\n",
      "ser 0.0160427807487\n",
      "melhor 0.0267379679144\n",
      "em 0.0427807486631\n",
      "tudo 0.0267379679144\n",
      "até 0.0160427807487\n",
      "pq 0.0160427807487\n",
      "eu 0.0160427807487\n",
      "sou 0.0160427807487\n",
      "melhor 0.0320855614973\n",
      "em 0.048128342246\n",
      "tudo 0.0320855614973\n",
      "né 0.0160427807487\n"
     ]
    }
   ],
   "source": [
    "#Now we'll go through the term list we compiled and update the score on our dictionary whenever we find a previous occurrence\n",
    "for item in one_list:\n",
    "    if item in uniqueScore.keys():\n",
    "        uniqueScore[item] += 1\n",
    "    else:\n",
    "        print 'Nothing to see here'\n",
    "        continue\n",
    "    print item, uniqueScore[item]/len(one_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tenho'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweetScore= []\n",
    "if str(tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet.append('aggression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#newList= [val for i in ]\n",
    "#Make list of values from dictionary\n",
    "newList= []\n",
    "for i in range(0,len(tweet)):\n",
    "    if tweet[i].encode('utf-8').strip() in scores.keys():\n",
    "        newList.append(scores[tweet[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em\n"
     ]
    }
   ],
   "source": [
    "i= 5\n",
    "tweet[i].encode('utf-8') in scores.keys()\n",
    "print str(tweet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[tweet[15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-104-5c219357738d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-104-5c219357738d>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [blankList.append(scores[tweet[i]]) if tweet[i].encode('utf-8').strip() in scores.keys() for i in range(0,len(tweet)) ]\u001b[0m\n\u001b[1;37m                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "blankList= []\n",
    "[blankList.append(scores[tweet[i]]) if tweet[i].encode('utf-8').strip() in scores.keys() for i in range(0,len(tweet)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tenho',\n",
       " u'mania',\n",
       " u'em',\n",
       " u'ser',\n",
       " u'melhor',\n",
       " u'em',\n",
       " u'tudo',\n",
       " u'at\\xe9',\n",
       " u'pq',\n",
       " u'eu',\n",
       " u'sou',\n",
       " u'melhor',\n",
       " u'em',\n",
       " u'tudo',\n",
       " u'n\\xe9',\n",
       " 'aggression']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we'll look at the happiest state code starting with the prob 2 code to read the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweetScores= []#Initialize list of total score per tweet\n",
    "\n",
    "tweet_index= 0#Initialize a counter to keep track of tweet we're on\n",
    "for line2 in tweet_file:\n",
    "    print 'Trying line: ', line2\n",
    "    try:#We'll try our entire process for all objects that are tweets and not 'delete' objects (which have no 'text' field)\n",
    "        tweet_index= tweet_index+1\n",
    "        dataDict= json.loads(line2)#Loads first 'line2', which is actually the first json object\n",
    "        #print 'Tweet index:', tweet_index, ',', 'Tweet text: '#, dataDict['text']\n",
    "        tweet= dataDict['place'].split()#Split text object into a list of terms\n",
    "        print tweet\n",
    "        \n",
    "        tweetList= []#Initialize list of scores for tweet \n",
    "        sumTweet= []#Initialize sum variable\n",
    "        for i in range(0,len(tweet)):#For item in list, if it is in dictionary, append value/score (from key:value pair) to tweetList. If not, append a 0 value instead\n",
    "            if tweet[i].encode('utf-8').strip() in scores.keys():\n",
    "                 tweetList.append(scores[tweet[i]])\n",
    "            else:\n",
    "                tweetList.append(0)\n",
    "        sumTweet= sum(tweetList)#Add scores of all terms in tweet\n",
    "        tweetScores.append(sumTweet)\n",
    "        print sumTweet#'Tweet score: ', sumTweet#Append this total score to a list with combined scores for all tweets\n",
    "    except:#If not, then:\n",
    "        print 'Has no text, skip entry'\n",
    "        continue\n",
    "\n",
    "    #==\n",
    "    #The following is not relevant anymore since it prints out 0 as length (reason is we finished reading file so by this point there are no more lines left)\n",
    "    #lines(sent_file)\n",
    "    #lines(tweet_file)\n",
    "    #print 'Our final score table for the tweets is: ', tweetScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-fff4d77459e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweet' is not defined"
     ]
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "reset_selective tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
